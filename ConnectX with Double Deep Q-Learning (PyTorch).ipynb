{"cells":[{"metadata":{},"cell_type":"markdown","source":"# About Reinforcement Learning and Deep Q-Learning\n> \"Reinforcement learning is an area of machine learning that is focused on training agents to take certain actions at certain states from within an environment to maximize rewards. DQN (Deep Q-Net) is a reinforcement learning algorithm where a deep learning model is built to find the actions an agent can take at each state.\" [*Siwei Xu's tutorial\n*](https://towardsdatascience.com/deep-reinforcement-learning-build-a-deep-q-network-dqn-to-play-cartpole-with-tensorflow-2-and-gym-8e105744b998)"},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"ToC\"></a>\n# Table of Contents\n1. [Install libraries](#install_libraries)\n1. [Import libraries](#import_libraries)\n1. [Define useful classes](#define_useful_classes)\n1. [Define helper-functions](#define_helper_functions)\n1. [Create ConnectX environment](#create_connectx_environment)\n1. [Configure hyper-parameters](#configure_hyper_parameters)\n1. [Train the agent](#train_the_agent)\n1. [Save weights](#save_weights)\n1. [Create an agent](#create_an_agent)\n1. [Evaluate the agent](#evaluate_the_agent)"},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"install_libraries\"></a>\n# Install libraries\n[Back to Table of Contents](#ToC)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install 'kaggle-environments>=0.1.6' > /dev/null 2>&1","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"import_libraries\"></a>\n# Import libraries\n[Back to Table of Contents](#ToC)"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport gym\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom kaggle_environments import evaluate, make","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"define_useful_classes\"></a>\n# Define useful classes\nNOTE: All classes here were copied from my previous [*kernel*](https://www.kaggle.com/phunghieu/connectx-with-deep-q-learning) and switched from using TF2.0 to PyTorch. If you prefer TF2.0, let check [ConnectX with Deep Q-Learning](https://www.kaggle.com/phunghieu/connectx-with-deep-q-learning) kernel.\n\n---\n[Back to Table of Contents](#ToC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ConnectX(gym.Env):\n    def __init__(self, switch_prob=0.5):\n        self.env = make('connectx', debug=False)\n        self.pair = [None, 'random']\n        self.trainer = self.env.train(self.pair)\n        self.switch_prob = switch_prob\n\n        # Define required gym fields (examples):\n        config = self.env.configuration\n        self.action_space = gym.spaces.Discrete(config.columns)\n        self.observation_space = gym.spaces.Discrete(config.columns * config.rows)\n\n    def switch_trainer(self):\n        self.pair = self.pair[::-1]\n        self.trainer = self.env.train(self.pair)\n\n    def step(self, action):\n        return self.trainer.step(action)\n    \n    def reset(self):\n        if np.random.random() < self.switch_prob:\n            self.switch_trainer()\n        return self.trainer.reset()\n    \n    def render(self, **kwargs):\n        return self.env.render(**kwargs)\n    \n    \nclass DeepModel(nn.Module):\n    def __init__(self, num_states, hidden_units, num_actions):\n        super(DeepModel, self).__init__()\n        self.hidden_layers = nn.ModuleList([])\n        for i in range(len(hidden_units)):\n            if i == 0:\n                self.hidden_layers.append(\n                    nn.Linear(num_states, hidden_units[i])\n                )\n            else:\n                self.hidden_layers.append(\n                    nn.Linear(hidden_units[i-1], hidden_units[i])\n                )\n        self.output_layer = nn.Linear(hidden_units[-1], num_actions)\n\n    def forward(self, x):\n        for layer in self.hidden_layers:\n            x = torch.sigmoid(layer(x))\n        x = self.output_layer(x)\n\n        return x\n\n\nclass DQN:\n    def __init__(self, num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr):\n        self.num_actions = num_actions\n        self.batch_size = batch_size\n        self.gamma = gamma\n        self.model = DeepModel(num_states, hidden_units, num_actions)\n        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n        self.criterion = nn.MSELoss()\n        self.experience = {'s': [], 'a': [], 'r': [], 's2': [], 'done': []} # The buffer\n        self.max_experiences = max_experiences\n        self.min_experiences = min_experiences\n\n    def predict(self, inputs):\n        return self.model(torch.from_numpy(inputs).float())\n\n    def train(self, TargetNet):\n        if len(self.experience['s']) < self.min_experiences:\n            # Only start the training process when we have enough experiences in the buffer\n            return 0\n\n        # Randomly select n experience in the buffer, n is batch-size\n        ids = np.random.randint(low=0, high=len(self.experience['s']), size=self.batch_size)\n        states = np.asarray([self.preprocess(self.experience['s'][i]) for i in ids])\n        actions = np.asarray([self.experience['a'][i] for i in ids])\n        rewards = np.asarray([self.experience['r'][i] for i in ids])\n\n        # Prepare labels for training process\n        states_next = np.asarray([self.preprocess(self.experience['s2'][i]) for i in ids])\n        dones = np.asarray([self.experience['done'][i] for i in ids])\n        # append \n        value_next_append=self.predict(states_next).detach().numpy()\n        best_actions_append = np.argmax(value_next_append, axis=1)\n        q_values_next_target_append = TargetNet.predict(states_next).detach().numpy()\n        actual_values = np.where(dones, rewards, rewards+self.gamma*q_values_next_target_append[np.arange(self.batch_size), best_actions_append])\n        \n        \n        '''\n        value_next = np.max(TargetNet.predict(states_next).detach().numpy(), axis=1)\n        actual_values = np.where(dones, rewards, rewards+self.gamma*value_next)\n        '''\n\n        actions = np.expand_dims(actions, axis=1)\n        actions_one_hot = torch.FloatTensor(self.batch_size, self.num_actions).zero_()\n        actions_one_hot = actions_one_hot.scatter_(1, torch.LongTensor(actions), 1)\n        selected_action_values = torch.sum(self.predict(states) * actions_one_hot, dim=1)\n        actual_values = torch.FloatTensor(actual_values)\n\n        self.optimizer.zero_grad()\n        loss = self.criterion(selected_action_values, actual_values)\n        loss.backward()\n        self.optimizer.step()\n\n    # Get an action by using epsilon-greedy\n    def get_action(self, state, epsilon):\n        if np.random.random() < epsilon:\n            return int(np.random.choice([c for c in range(self.num_actions) if state.board[c] == 0]))\n        else:\n            prediction = self.predict(np.atleast_2d(self.preprocess(state)))[0].detach().numpy()\n            for i in range(self.num_actions):\n                if state.board[i] != 0:\n                    prediction[i] = -1e7\n            return int(np.argmax(prediction))\n\n    # Method used to manage the buffer\n    def add_experience(self, exp):\n        if len(self.experience['s']) >= self.max_experiences:\n            for key in self.experience.keys():\n                self.experience[key].pop(0)\n        for key, value in exp.items():\n            self.experience[key].append(value)\n\n    def copy_weights(self, TrainNet):\n        self.model.load_state_dict(TrainNet.state_dict())\n\n    def save_weights(self, path):\n        torch.save(self.model.state_dict(), path)\n\n    def load_weights(self, path):\n        self.model.load_state_dict(torch.load(path))\n    \n    # Each state will consist of the board and the mark\n    # in the observations\n    def preprocess(self, state):\n        result = state.board[:]\n        result.append(state.mark)\n\n        return result","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"define_helper_functions\"></a>\n# Define helper-functions\n[Back to Table of Contents](#ToC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def play_game(env, TrainNet, TargetNet, epsilon, copy_step):\n    rewards = 0\n    iter = 0\n    done = False\n    observations = env.reset()\n    while not done:\n        # Using epsilon-greedy to get an action\n        action = TrainNet.get_action(observations, epsilon)\n\n        # Caching the information of current state\n        prev_observations = observations\n\n        # Take action\n        observations, reward, done, _ = env.step(action)\n\n        # Apply new rules\n        if done:\n            if reward == 1: # Won\n                reward = 20\n            elif reward == 0: # Lost\n                reward = -20\n            else: # Draw\n                reward = 10\n        else:\n#             reward = -0.05 # Try to prevent the agent from taking a long move\n\n            # Try to promote the agent to \"struggle\" when playing against negamax agent\n            # as Magolor's (@magolor) idea\n            reward = 0.5\n\n        rewards += reward\n\n        # Adding experience into buffer\n        exp = {'s': prev_observations, 'a': action, 'r': reward, 's2': observations, 'done': done}\n        TrainNet.add_experience(exp)\n\n        # Train the training model by using experiences in buffer and the target model\n        ## modify:\n        #TrainNet.train(TargetNet, )\n        TrainNet.train(TargetNet)\n        iter += 1\n        if iter % copy_step == 0:\n            # Update the weights of the target model when reaching enough \"copy step\"\n            TargetNet.copy_weights(TrainNet)\n    return rewards","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"create_connectx_environment\"></a>\n# Create ConnectX environment\n[Back to Table of Contents](#ToC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"env = ConnectX()","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"configure_hyper_parameters\"></a>\n# Configure hyper-parameters\n[Back to Table of Contents](#ToC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"gamma = 0.99\ncopy_step = 25\nhidden_units = [128, 128, 128, 128, 128]\nmax_experiences = 30000\nmin_experiences = 1000\nbatch_size = 32\nlr = 1e-2\nepsilon = 0.5\ndecay = 0.9999\nmin_epsilon = 0.1\nepisodes = 50000\n\nprecision = 7","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"train_the_agent\"></a>\n# Train the agent\n[Back to Table of Contents](#ToC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nnum_states = env.observation_space.n + 1\nnum_actions = env.action_space.n\n\nall_total_rewards = np.empty(episodes)\nall_avg_rewards = np.empty(episodes) # Last 100 steps\nall_epsilons = np.empty(episodes)\n\n# Initialize models\nTrainNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\nTargetNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pbar = tqdm(range(episodes))\nfor n in pbar:\n    epsilon = max(min_epsilon, epsilon * decay)\n    total_reward = play_game(env, TrainNet, TargetNet, epsilon, copy_step)\n    all_total_rewards[n] = total_reward\n    avg_reward = all_total_rewards[max(0, n - 100):(n + 1)].mean()\n    all_avg_rewards[n] = avg_reward\n    all_epsilons[n] = epsilon\n\n    pbar.set_postfix({\n        'episode reward': total_reward,\n        'avg (100 last) reward': avg_reward,\n        'epsilon': epsilon\n    })","execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=50000.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42f1530b41e14a209902c1b42f4b3cf9"}},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-5754319d01e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_epsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTargetNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mall_total_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mavg_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_total_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-2dba95da6291>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(env, TrainNet, TargetNet, epsilon, copy_step)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m## modify:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m#TrainNet.train(TargetNet, )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mTrainNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTargetNet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0miter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcopy_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-ba01f21818b2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, TargetNet)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mvalue_next_append\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTargetNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mbest_actions_append\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_next_append\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mq_values_next_target_append\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTargetNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mactual_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mq_values_next_target_append\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_actions_append\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-ba01f21818b2>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTargetNet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.plot(all_total_rewards)\n# plt.xlabel('Episode')\n# plt.ylabel('Total rewards')\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(all_avg_rewards)\nplt.xlabel('Episode')\nplt.ylabel('Avg rewards (100)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(all_epsilons)\nplt.xlabel('Episode')\nplt.ylabel('Epsilon')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"save_weights\"></a>\n# Save weights\n[Back to Table of Contents](#ToC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"TrainNet.save_weights('./weights.pth')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"create_an_agent\"></a>\n# Create an agent\n[Back to Table of Contents](#ToC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"fc_layers = []\n\n# Get all hidden layers' weights\nfor i in range(len(hidden_units)):\n    fc_layers.extend([\n        TrainNet.model.hidden_layers[i].weight.T.tolist(), # weights\n        TrainNet.model.hidden_layers[i].bias.tolist() # bias\n    ])\n\n# Get output layer's weights\nfc_layers.extend([\n    TrainNet.model.output_layer.weight.T.tolist(), # weights\n    TrainNet.model.output_layer.bias.tolist() # bias\n])\n\n# Convert all layers into usable form before integrating to final agent\nfc_layers = list(map(\n    lambda x: str(list(np.round(x, precision))) \\\n        .replace('array(', '').replace(')', '') \\\n        .replace(' ', '') \\\n        .replace('\\n', ''),\n    fc_layers\n))\nfc_layers = np.reshape(fc_layers, (-1, 2))\n\n# Create the agent\nmy_agent = '''def my_agent(observation, configuration):\n    import numpy as np\n\n'''\n\n# Write hidden layers\nfor i, (w, b) in enumerate(fc_layers[:-1]):\n    my_agent += '    hl{}_w = np.array({}, dtype=np.float32)\\n'.format(i+1, w)\n    my_agent += '    hl{}_b = np.array({}, dtype=np.float32)\\n'.format(i+1, b)\n# Write output layer\nmy_agent += '    ol_w = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][0])\nmy_agent += '    ol_b = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][1])\n\nmy_agent += '''\n    state = observation.board[:]\n    state.append(observation.mark)\n    out = np.array(state, dtype=np.float32)\n\n'''\n\n# Calculate hidden layers\nfor i in range(len(fc_layers[:-1])):\n    my_agent += '    out = np.matmul(out, hl{0}_w) + hl{0}_b\\n'.format(i+1)\n    my_agent += '    out = 1/(1 + np.exp(-out))\\n' # Sigmoid function\n# Calculate output layer\nmy_agent += '    out = np.matmul(out, ol_w) + ol_b\\n'\n\nmy_agent += '''\n    for i in range(configuration.columns):\n        if observation.board[i] != 0:\n            out[i] = -1e7\n\n    return int(np.argmax(out))\n    '''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('submission_pytorch.py', 'w') as f:\n    f.write(my_agent)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"evaluate_the_agent\"></a>\n# Evaluate the agent\n[Back to Table of Contents](#ToC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from submission_pytorch import my_agent","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mean_reward(rewards):\n    return sum(r[0] for r in rewards) / sum(r[0] + r[1] for r in rewards)\n\n# Run multiple episodes to estimate agent's performance.\nprint(\"My Agent vs. Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=10)))\nprint(\"My Agent vs. Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))\nprint(\"Random Agent vs. My Agent:\", mean_reward(evaluate(\"connectx\", [\"random\", my_agent], num_episodes=10)))\nprint(\"Negamax Agent vs. My Agent:\", mean_reward(evaluate(\"connectx\", [\"negamax\", my_agent], num_episodes=10)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}